import streamlit as st
import requests
from bs4 import BeautifulSoup
import urllib.parse
import pandas as pd
import time

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="JW Library Search",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main {
        background-color: #f8f9fa;
    }
    .stApp {
        font-family: 'Roboto', sans-serif;
    }
    .search-result {
        background-color: white;
        border-radius: 10px;
        padding: 20px;
        margin-bottom: 15px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        transition: transform 0.2s;
    }
    .search-result:hover {
        transform: translateY(-3px);
        box-shadow: 0 4px 8px rgba(0,0,0,0.15);
    }
    .result-title {
        color: #1f77b4;
        font-weight: bold;
        font-size: 18px;
        margin-bottom: 5px;
    }
    .result-publication {
        color: #777;
        font-size: 14px;
        font-style: italic;
        margin-bottom: 8px;
    }
    .result-snippet {
        color: #333;
        font-size: 15px;
        line-height: 1.5;
    }
    .placeholder {
        background: linear-gradient(90deg, #f2f2f2 25%, #e6e6e6 50%, #f2f2f2 75%);
        background-size: 200% 100%;
        animation: loading 1.5s infinite;
        border-radius: 4px;
        height: 100px;
        margin-bottom: 15px;
    }
    @keyframes loading {
        0% {
            background-position: 200% 0;
        }
        100% {
            background-position: -200% 0;
        }
    }
    .sidebar-header {
        margin-bottom: 30px;
    }
</style>
""", unsafe_allow_html=True)

# ------------------------------
# â‘  è¨€èªã«åˆã‚ã›ãŸãƒ™ãƒ¼ã‚¹URLã®è¨­å®š
# ------------------------------
def get_base_url(lang):
    """
    è¨€èªã”ã¨ã«ãƒ™ãƒ¼ã‚¹URLã‚’è¿”ã—ã¾ã™ã€‚
    æ—¥æœ¬èªã®å ´åˆ: "https://wol.jw.org/ja/wol/s/r7/lp-j"
    è‹±èªã®å ´åˆ:   "https://wol.jw.org/en/wol/s/r1/lp-e"
    """
    urls = {
        "ja": "https://wol.jw.org/ja/wol/s/r7/lp-j",
        "en": "https://wol.jw.org/en/wol/s/r1/lp-e"
    }
    # æŒ‡å®šã•ã‚ŒãŸè¨€èªãŒç„¡ã„å ´åˆã¯æ—¥æœ¬èªç‰ˆã‚’è¿”ã™
    return urls.get(lang, urls["ja"])

# ------------------------------
# â‘¡ æ¤œç´¢å‡¦ç†ï¼ˆãƒšãƒ¼ã‚¸å˜ä½ã§yieldï¼‰
# ------------------------------
def _fetch_and_parse_page(keyword, page_num, lang="ja", sort="occ"):
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ãƒšãƒ¼ã‚¸ç•ªå·ã€è¨€èªã€ã‚½ãƒ¼ãƒˆé †ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã€
    çµæœï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼ãƒªãƒ³ã‚¯ï¼ã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼å‡ºç‰ˆç‰©æƒ…å ±ï¼‰ã®ãƒªã‚¹ãƒˆã¨æ¬¡ãƒšãƒ¼ã‚¸ã®æœ‰ç„¡ã‚’è¿”ã—ã¾ã™ã€‚
    """
    base_url = get_base_url(lang)
    params = {
        "q": keyword,
        "st": "a",
        "p": "par",
        "r": sort,
        "pg": str(page_num),
    }
    
    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        items = []
        # æ—§å½¢å¼ã®å ´åˆï¼ˆul.results.resultContentDocumentï¼‰
        result_blocks = soup.select("ul.results.resultContentDocument")
        # ã‚«ãƒ¼ãƒ‰å½¢å¼ã®å ´åˆã¯ li.navCard ã‚’å¯¾è±¡
        if not result_blocks:
            result_blocks = soup.select("li.navCard")
        
        for rb in result_blocks:
            # ã€ã‚±ãƒ¼ã‚¹1ã€‘æ—§å½¢å¼
            caption_li = rb.select_one("li.caption")
            if caption_li:
                link_tag = caption_li.select_one("a.lnk")
                if link_tag:
                    title = link_tag.get_text(strip=True)
                    # ç›¸å¯¾ãƒ‘ã‚¹ã‚’çµ¶å¯¾URLã«å¤‰æ›
                    link = urllib.parse.urljoin(base_url, link_tag.get("href", "").strip())
                else:
                    title, link = "", ""
                snippet_li = rb.select_one("li.searchResult")
                if snippet_li:
                    doc_div = snippet_li.select_one("div.document")
                    snippet = doc_div.get_text(" ", strip=True) if doc_div else ""
                else:
                    snippet = ""
                publication = ""
            # ã€ã‚±ãƒ¼ã‚¹2ã€‘ã‚«ãƒ¼ãƒ‰å½¢å¼
            elif rb.select_one("div.cardTitleBlock"):
                title_div = rb.select_one("div.cardLine1")
                title = title_div.get_text(strip=True) if title_div else ""
                alt_title_div = rb.select_one("div.cardLine2")
                if not title and alt_title_div:
                    title = alt_title_div.get_text(strip=True)
                link_tag = rb.select_one("a")
                if link_tag:
                    link = urllib.parse.urljoin(base_url, link_tag.get("href", "").strip())
                else:
                    link = ""
                snippet = ""
                detail_div = rb.select_one("div.cardTitleDetail")
                publication = detail_div.get_text(strip=True) if detail_div else ""
            else:
                title, link, snippet, publication = "", "", "", ""
            
            if title and link:  # æœ‰åŠ¹ãªçµæœã®ã¿è¿½åŠ 
                items.append({
                    "title": title,
                    "link": link,
                    "snippet": snippet,
                    "publication": publication
                })
        
        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ã®å–å¾—ï¼ˆè©²å½“è¦ç´ ãŒç„¡ã‘ã‚Œã°æ¬¡ãƒšãƒ¼ã‚¸ç„¡ã—ã¨åˆ¤æ–­ï¼‰
        try:
            total_str = soup.select_one("#searchResultsTotal")["value"]
            page_size_str = soup.select_one("#searchResultsPageSize")["value"]
            current_page_str = soup.select_one("#searchResultsPageNumber")["value"]
            total = int(total_str)
            page_size = int(page_size_str)
            current_page = int(current_page_str)
            total_pages = (total + page_size - 1) // page_size
            has_next = (current_page < total_pages)
            result_info = {
                "total": total,
                "current_page": current_page,
                "total_pages": total_pages
            }
        except (TypeError, KeyError):
            has_next = False
            result_info = {
                "total": len(items),
                "current_page": 1,
                "total_pages": 1
            }
        
        return {"items": items, "has_next": has_next, "result_info": result_info}
    
    except Exception as e:
        st.error(f"æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return {"items": [], "has_next": False, "result_info": {"total": 0, "current_page": 1, "total_pages": 1}}

# ------------------------------
# Streamlit UIå‡¦ç†
# ------------------------------
def display_search_result(item):
    """æ¤œç´¢çµæœã®1é …ç›®ã‚’è¡¨ç¤ºã™ã‚‹"""
    st.markdown(f"""
    <div class="search-result">
        <div class="result-title">{item['title']}</div>
        <div class="result-publication">{item['publication']}</div>
        <div class="result-snippet">{item['snippet']}</div>
        <a href="{item['link']}" target="_blank">è¨˜äº‹ã‚’èª­ã‚€</a>
    </div>
    """, unsafe_allow_html=True)

def display_loading_animation(num_placeholders=3):
    """ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’è¡¨ç¤º"""
    loading_container = st.container()
    with loading_container:
        for _ in range(num_placeholders):
            st.markdown('<div class="placeholder"></div>', unsafe_allow_html=True)
    return loading_container

# ------------------------------
# Streamlitã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†
# ------------------------------
def main():
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.markdown('<div class="sidebar-header"></div>', unsafe_allow_html=True)
        st.image("https://assetsnffrgf-a.akamaihd.net/assets/m/802013135/univ/art/802013135_univ_lsr_xl.jpg", width=200)
        st.markdown("## JW Library Search")
        st.markdown("---")
        
        # æ¤œç´¢è¨­å®š
        keyword = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", placeholder="ä¾‹: è–æ›¸, ä¿¡ä»°, å¸Œæœ›...")
        lang = st.selectbox("è¨€èªã‚’é¸æŠ", ["ja", "en"], format_func=lambda x: "æ—¥æœ¬èª" if x == "ja" else "English")
        sort_options = {
            "newest": "æœ€æ–°é †",
            "occ": "é–¢é€£æ€§é †"
        }
        sort = st.selectbox("ä¸¦ã³é †", list(sort_options.keys()), format_func=lambda x: sort_options[x])
        max_pages = st.slider("æœ€å¤§å–å¾—ãƒšãƒ¼ã‚¸æ•°", 1, 10, 3)
        
        search_button = st.button("æ¤œç´¢", use_container_width=True)
        
        st.markdown("---")
        st.markdown("### About")
        st.markdown("""
        ã“ã®ã‚¢ãƒ—ãƒªã¯WatchTower Online Libraryã®æ¤œç´¢ã‚’è¡Œã„ã¾ã™ã€‚
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã€æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚
        """)
        
        st.markdown("### Export")
        export_format = st.selectbox("ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼", ["CSV", "Excel"])
        
        # æ¤œç´¢çµæœãŒã‚ã‚‹å ´åˆã®ã¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
        if 'all_results' in st.session_state and len(st.session_state.all_results) > 0:
            if st.button("æ¤œç´¢çµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", use_container_width=True):
                df = pd.DataFrame(st.session_state.all_results)
                if export_format == "CSV":
                    csv = df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        label="CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=csv,
                        file_name=f"search_results_{keyword}_{lang}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
                else:
                    output = pd.ExcelWriter(f"search_results_{keyword}_{lang}.xlsx", engine='xlsxwriter')
                    df.to_excel(output, index=False, sheet_name='æ¤œç´¢çµæœ')
                    output.save()
                    st.success("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢
    st.title("JW Library Search Tool")
    
    # åˆæœŸè¡¨ç¤º
    if 'all_results' not in st.session_state:
        st.session_state.all_results = []
    
    # æ¤œç´¢å®Ÿè¡Œ
    if search_button and keyword:
        st.session_state.all_results = []  # çµæœã‚’ãƒªã‚»ãƒƒãƒˆ
        
        # æ¤œç´¢é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        st.info(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã§æ¤œç´¢ã‚’é–‹å§‹ã—ã¾ã™...")
        
        # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º
        loading = display_loading_animation()
        
        # æ¤œç´¢å®Ÿè¡Œ
        page_num = 1
        total_pages = max_pages  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        while page_num <= max_pages:
            # æ¤œç´¢ã‚’å®Ÿè¡Œ
            page_data = _fetch_and_parse_page(keyword, page_num, lang=lang, sort=sort)
            
            # çµæœã‚’ä¿å­˜
            st.session_state.all_results.extend(page_data["items"])
            
            # åˆè¨ˆãƒšãƒ¼ã‚¸æ•°ã‚’æ›´æ–°
            if page_num == 1:
                total_pages = min(max_pages, page_data["result_info"]["total_pages"])
                if total_pages == 0:
                    break
            
            # é€²æ—çŠ¶æ³ã‚’è¡¨ç¤º
            progress = page_num / total_pages
            st.progress(progress)
            
            # æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒãªã‘ã‚Œã°çµ‚äº†
            if not page_data["has_next"]:
                break
                
            page_num += 1
            time.sleep(0.5)  # ã‚µãƒ¼ãƒãƒ¼è² è·ã‚’è€ƒæ…®

        # ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’éè¡¨ç¤º
        loading.empty()
        
        # æ¤œç´¢çµæœæ¦‚è¦
        if len(st.session_state.all_results) > 0:
            st.success(f"æ¤œç´¢å®Œäº†ï¼ {len(st.session_state.all_results)} ä»¶ã®çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
        else:
            st.warning("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚")
    
    # æ¤œç´¢çµæœã®è¡¨ç¤º
    if 'all_results' in st.session_state and len(st.session_state.all_results) > 0:
        # çµæœãƒªã‚¹ãƒˆã‚’DataFrameã«å¤‰æ›
        df = pd.DataFrame(st.session_state.all_results)
        
        # ã‚¿ãƒ–ã§è¡¨ç¤ºæ–¹æ³•ã‚’åˆ‡ã‚Šæ›¿ãˆ
        tab1, tab2 = st.tabs(["ã‚«ãƒ¼ãƒ‰è¡¨ç¤º", "ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º"])
        
        with tab1:
            # ã‚«ãƒ¼ãƒ‰è¡¨ç¤º
            for item in st.session_state.all_results:
                display_search_result(item)
        
        with tab2:
            # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
            st.dataframe(
                df[["title", "publication", "link"]],
                column_config={
                    "title": "ã‚¿ã‚¤ãƒˆãƒ«",
                    "publication": "å‡ºç‰ˆç‰©",
                    "link": st.column_config.LinkColumn("ãƒªãƒ³ã‚¯")
                },
                hide_index=True,
                use_container_width=True
            )
    elif keyword and search_button:
        st.info("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã€æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()
